{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab 4-1: Data preprocessing. Building a regression model.\n",
    "\n",
    "During this lab session we will train a simple regression model to predict the solubility of chemical molecules. We will use the `RDKit` library to calculate molecular descriptors from SMILES strings, and the `scikit-learn` library to implement the model. Then, we will apply some simple data preprocessing, implement a featurizer for automatic input data preparation, and wrap the whole process in a simple, user-friendly web app using `streamlit`.\n",
    "\n",
    "---\n",
    "\n",
    "## Loading the dataset\n",
    "\n",
    "We will use the [AqSolDB](https://doi.org/10.1038/s41597-019-0151-1) dataset, which contains experimental solubility values for a large number of chemical molecules. The dataset is stored in the `data/aqsol.csv` file and contains the following columns:\n",
    "- `Name`: the name of the molecule\n",
    "- `SMILES`: the SMILES string of the molecule\n",
    "- `Solubility`: logS value (base 10 logarithm of the solubility in mol/L)\n"
   ],
   "id": "383075d4f797ffc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/aqsol.csv')\n",
    "df.head()"
   ],
   "id": "49a509e53faea6b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calculating molecular descriptors with RDKit\n",
    "\n",
    "**RDKit is a collection of cheminformatics tools, widely used to analyze and process chemical data.** We will use some of its functionalities to calculate molecular descriptors for the molecules in our dataset, and use the descriptors as input features for a regression model that will learn to predict the solubility of the molecules from SMILES strings. \n",
    "\n",
    "For this task will use only one submodule of RDKit, `rdkit.Chem.rdMolDescriptors`, which contains functions for calculating the molecular descriptors we are interested in. Feel free to explore RDKit yourself by reading [rdMolDescriptors documentation](https://www.rdkit.org/docs/source/rdkit.Chem.rdMolDescriptors.html) and [RDKit documentation](https://www.rdkit.org/docs/index.html).\n",
    "\n",
    "The most important RDKit class is `Mol` which represents a molecule with its atoms, bonds, spatial conformation, etc. Most RDKit functions, including those for calculating molecular descriptors, take a `Mol` object as input. If you have a SMILES string, you can create a `Mol` object using the `Chem.MolFromSmiles` function, as shown below:\n",
    "\n",
    "```python\n",
    "from rdkit import Chem\n",
    "\n",
    "smiles = 'C1C(=O)NC2=C(C=C(C=C2)[N+](=O)[O-])C(=N1)C3=CC=CC=C3'\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "```\n",
    "\n",
    "From here, you can calculate various molecular descriptors that will be used as input features for our regression model. Below is an example of how to calculate some common molecular descriptors using RDKit, implemented in the `get_descriptors` function."
   ],
   "id": "3ab12a34250cf4e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import Crippen\n",
    "from rdkit import RDLogger  \n",
    "RDLogger.DisableLog('rdApp.*') # Disabling rdkit warnings\n",
    "\n",
    "def get_descriptors(smiles_list):\n",
    "\n",
    "    # Create a dataframe to store the descriptors (for convenience, could be done differently)\n",
    "    df = pd.DataFrame({'SMILES': smiles_list})\n",
    "    df['mol'] = df['SMILES'].apply(Chem.MolFromSmiles)\n",
    "\n",
    "    # Now we can calculate the molecular descriptors themselves (I chose a few common ones here)\n",
    "    df['mol_wt'] = df['mol'].apply(rdMolDescriptors.CalcExactMolWt)             # Molecular weight\n",
    "    df['logp'] = df['mol'].apply(Crippen.MolLogP)                               # LogP (lipophilicity)\n",
    "    df['num_heavy_atoms'] = df['mol'].apply(rdMolDescriptors.CalcNumHeavyAtoms) # Number of heavy atoms\n",
    "    df['num_HBD'] = df['mol'].apply(rdMolDescriptors.CalcNumHBD)                # Number of hydrogen bond donors\n",
    "    df['num_HBA'] = df['mol'].apply(rdMolDescriptors.CalcNumHBA)                # Number of hydrogen bond acceptors\n",
    "    df['aromatic_rings'] = df['mol'].apply(rdMolDescriptors.CalcNumAromaticRings) # Number of aromatic rings\n",
    "\n",
    "    # Now we select only the relevant columns (descriptors) and return the dataframe\n",
    "    df = df[['mol_wt', 'logp', 'num_heavy_atoms', 'num_HBD', 'num_HBA', 'aromatic_rings']]\n",
    "    return df\n",
    "\n",
    "df = get_descriptors(df['SMILES'])\n",
    "df.head()"
   ],
   "id": "7d60f4277dd88387",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 1: Split the data and extract features (1 point)\n",
    "\n",
    "Now that we know how to calculate molecular descriptors, we can use them as input features for our regression model. Our target variable will be the `Solubility` column from the original dataset.\n",
    "\n",
    "1. Split the dataset into a training set (80%) and a test set (20%) using the `train_test_split` function from `sklearn.model_selection`. Store the resulting dataframes as `train` and `test`.\n",
    "2. Extract the molecular descriptors for both the training and test sets using the `get_descriptors` function defined above and store them in the `X_train` and `X_test` variables, respectively.\n",
    "3. Store the target variable (logS values) in the `y_train` and `y_test` variables for both sets."
   ],
   "id": "44b96d1421ed6e46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the aqsol dataset into training and test sets\n",
    "\n",
    "train, test = ..."
   ],
   "id": "cf2da0c4db1756bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2: Test some regression models (1 points)\n",
    "\n",
    "You already know the `scikit-learn` library, as we used it to build some classifier models in the previous labs. Now, we will use it to build a regression model. Linear regression is the simplest regression model, and it is a good starting point for regression problems. It is implemented in scikit-learn as `LinearRegression`. You should also try a more complex model, such as `SVR` (Support Vector Regression) and compare the results.\n",
    "\n",
    "1. Train a `LinearRegression` model on the training set. Report $RMSE$ (Root Mean Squared Error) and $R^2$ score on the train and test sets.\n",
    "2. Train an `SVR` model on the training set. Report $RMSE$ and $R^2$ score on the train and test sets. Use the default hyperparameters for now."
   ],
   "id": "2c6e695e991ef6e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "...\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "..."
   ],
   "id": "ddd091b6b7bf3023",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature scaling - standardization and normalization\n",
    "\n",
    "**Standardization** of datasets is a common requirement for many machine learning estimators implemented in scikit-learn. If some features of our data have very different scales (for example, one feature is in the range $[0, 1]$ and another can potentially be any positive number), some models might consider the feature with larger numerical values to be more important. This can be a problem, as we want our model to be able to learn from all features equally.\n",
    "\n",
    "Standardization transform our data in such a way that its distribution will have a mean value $\\mu = 0$ and standard deviation $\\sigma = 1$. We can achieve this by using the `StandardScaler` from the `sklearn` library.\n",
    "\n",
    "Another common preprocessing step is **normalization**. In this case, the data is scaled to a fixed range, usually $[0, 1]$. The motivation to use this scaling method includes preserving zeros in sparse data. For example, when making predictions about the expected outcome of an anticancer therapy, the value of $0$ observed tumors in a patient is probably much more informative for the predictive model than any other number of observed tumors alone. We can scale data to a certain range by using the `MinMaxScaler` from the `sklearn` library.\n",
    "\n",
    "You can read more about those two feature scaling methods in the [Scikit-learn preprocessing docs](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
    "\n",
    "<center>\n",
    "    <img src=\"imgs/logp-std.png\" width=\"500\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "**Remember that the scaler should be fitted only on the training set, and then used to transform both the training and test sets.** If we first transform the entire dataset and then split it into training and test sets, we are leaking the information about **test set data distribution** to the model, which would lead to overly optimistic results.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "## Exercise 3: Preprocess the input features (2 points)\n",
    "\n",
    "1. Standardize the input features using the `StandardScaler` from `sklearn`. **Fit the scaler on the training set features and then transform both the training and test features**.\n",
    "2. Train both linear regression and SVR models on the standardized training set. Report $RMSE$ and $R^2$ score on the train and test sets. How do the results compare to the previous models? Which model benefits from standardization?"
   ],
   "id": "7ba17e8dbfa3cfdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_scaled = ...\n",
    "X_test_scaled = ..."
   ],
   "id": "b6b760550de9e5e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 5: Make predictions on new data (1 point)\n",
    "\n",
    "Now that we have a trained regression model, we can use it to predict the solubility of new molecules. We will use a new dataset of tryptamine derivatives for this task. The dataset is stored in the `data/tryptamines.csv` file and contains the following columns:\n",
    "- `Name`: the name of the molecule\n",
    "- `SMILES`: the SMILES string of the molecule\n",
    "Below is some code to load the dataset and visualize the molecules.\n",
    "\n",
    "Your task is to:\n",
    "1. Extract the features using the same molecular descriptors we used before\n",
    "2. Standardize them using the scaler fitted on the training set\n",
    "3. Use the trained regression model (linear regression or svm) to predict the solubility of the molecules"
   ],
   "id": "f3ce4926e3ad37a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "tryptamines = pd.read_csv('data/tryptamines.csv')   # Load a new dataset\n",
    "tryptamines.head()"
   ],
   "id": "8b8298f1fd14f03f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Draw molecules present in the tryptamines dataset\n",
    "\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import Chem\n",
    "\n",
    "mols = [Chem.MolFromSmiles(smiles) for smiles in tryptamines['SMILES']]\n",
    "labels = tryptamines['Name'].tolist()\n",
    "Draw.MolsToGridImage(mols, molsPerRow=3, subImgSize=(300,300), legends=labels)"
   ],
   "id": "890fa31949350626",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "... # Extract features, standardize them, make predictions",
   "id": "487bacd1f92bf92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 6: Featurizer class (2 points)\n",
    "\n",
    "All the data manipulations, including the generation of features (molecular descriptors) and scaling we did completely by hand. As the project grows, this approach becomes unmanageable and error-prone. We could hide all these data preprocessing steps inside a class, so that we can reuse it easily whenever we need to extract features from a list of SMILES strings.\n",
    "\n",
    "Let's encapsulate all the steps in a single class called `Featurizer`. \n",
    "\n",
    "**Featurizer is a piece of code which takes care of extracting the features and preparing them** in such a way that they can be used as an input for a ML model. In this fashion, our featurizer will be able to take a list of SMILES strings and return a dataframe with the molecular descriptors, ready to be used in any regression model.\n",
    "\n",
    "The `Featurizer` class should implement the following methods:\n",
    "\n",
    "1. `__init__(self, smiles_list)`\n",
    "\n",
    "    The constructor method should:\n",
    "    * Store a `StandardScaler` object as an attribute of the class.\n",
    "    * Take an array of **training set** SMILES strings as input and calculate molecular descriptors for each molecule.\n",
    "    * Fit the scaler to the calculated descriptors of the training set.\n",
    "\n",
    "2. `get_descriptors(self, smiles_list)`\n",
    "\n",
    "   This method should take an array of SMILES strings as input and return a dataframe with the molecular descriptors calculated with RDKit. You can reuse the `get_descriptors` function defined above.\n",
    "\n",
    "3. `featurize(self, smiles_list)`\n",
    "    \n",
    "   This method should take an array of SMILES strings as input and return a dataframe containing molecular descriptors calculated with RDKit and scaled using the scaler fitted on the training set.\n",
    "\n",
    "### Here is an example of how our `Featurizer` class should be used\n",
    "```python\n",
    "# Create an instance of the Featurizer class\n",
    "featurizer = Featurizer(X_train['SMILES'])\n",
    "\n",
    "# Say we want to predict the solubility of the following molecules\n",
    "some_SMILES = ['CC(=O)Oc1ccccc1C(=O)O',     \n",
    "               'C1=CC(=C(C=C1[C@H](CN)O)O)O',\n",
    "               'C1=NC2=C(C(=N1)N)N=CN2C3C(C(C(O3)CO)O)O']\n",
    "\n",
    "# Extract the features\n",
    "X = featurizer.featurize(some_SMILES)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "```\n"
   ],
   "id": "e7788a1513ee0f54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Featurizer:\n",
    "    def __init__(self, smiles_list):\n",
    "        ...\n",
    "    \n",
    "    def featurize(self, smiles_list):\n",
    "        ..."
   ],
   "id": "fdba06d2dc54ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test your featurizer\n",
    "\n",
    "tryptamines = pd.read_csv('data/tryptamines.csv')   # Load a new dataset\n",
    "tryptamines.head()"
   ],
   "id": "8054c47c9dab6f61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "featurizer = Featurizer(...) # Pass the training set SMILES strings to the constructor\n",
    "X = featurizer.featurize(tryptamines['SMILES'])   # Extract features for the new dataset\n",
    "X.head() # Take look at the generated features"
   ],
   "id": "edb8e9765cba0f25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predict the solubility of the new molecules\n",
    "\n",
    "y_pred = svr.predict(X)  # Use the linear regression model to predict the solubility\n",
    "\n",
    "tryptamines['Solubility'] = y_pred  # Add the predictions to the dataframe\n",
    "tryptamines.head()"
   ],
   "id": "3c47cf735cf5cb1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Cross-validation and grid search\n",
    "\n",
    "Now that we have a working regression model, we can try to improve its performance by tuning the hyperparameters. Last time we tuned the hyperparameters by hand, but as you may guess, that is not the most efficient way to do it. What we can (and will) do is employ a **search algorithm** to find the best hyperparameters automatically. This piece of code will try different hyperparameters and return the best combination.\n",
    "\n",
    "Last time we introduced the concept of a **validation set**, which is a subset of the training set used to evaluate the model's performance during hyperparameter tuning. To tune the hyperparameters of a classifier model we cut out a part of the training set and used the accuracy on this validation set as a metric to evaluate the model's performance.\n",
    "\n",
    "In practice, when tuning hyperparameters of a machine learning model, we do not usually create a single validation set. Instead, we employ a srategy called **cross-validation**. In $k$-fold cross-validation, the process of training and evaluating the model is repeated $k$ times, with each repetition using a different validation set. Thanks to this strategy, each data point present in the original training set will have the chance to be included in a validation sets. The metrics reported from $k$ folds are then averaged to get a more reliable estimate of the model's performance.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/cross-validation.png\" width=\"800\">\n",
    "</center>\n",
    "\n",
    "Luckily, we do not have to implement this algorithm ourselves, as scikit-learn provides a `GridSearchCV` class that can help us with this task. It performs an exhaustive search over a specified parameter grid and evaluates the model's performance using $k$-fold cross-validation. By default, `GridSearchCV` uses a 5-fold cross-validation, which is a common choice for $k$."
   ],
   "id": "8f535942fbf64684"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "# We will define a grid of hyperparameters as a dictionary. The keys are the hyperparameter names, and the values are lists of possible values to try.\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1, 5, 10, 20, 50, 100],\n",
    "    'epsilon': [0.001, 0.01, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    svr, # the model\n",
    "    param_grid, # the grid of hyperparameters\n",
    "    verbose=2 # print the progress\n",
    ")\n",
    "\n",
    "svr = grid_search.fit(X_train, y_train) # GridSearchCV.fit() returns the best model, and we can save it to a new variable"
   ],
   "id": "90aeecb460602ed6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Report the best hyperparameters and RMSE on the testing set\n",
    "\n",
    "print('Best hyperparameters:', svr.get_params())\n",
    "\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "print('-'*50)\n",
    "print('Testing set rmse:', mean_squared_error(y_test, y_pred))"
   ],
   "id": "ebc8f7c1b8fb2afe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *Randomized search\n",
    "\n",
    "`RandomizedSearchCV` is an alternative to `GridSearchCV`. Instead of trying all possible combinations of fixed hyperparameters, it samples a fixed number of hyperparameter settings from specified probability distributions. Although you may be hesitant to use it, as it is not an exhaustive search, it is often more efficient than grid search. Randomized search is especially useful when we have many hyperparameters to tune, and we are not sure which ones are the most important. Take a look at the figure below and try to understand why this is the case.\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/grid-vs-random.png\", width=\"600\">\n",
    "</center>\n",
    "\n",
    "Randomized search is implemented in the same way as grid search, but with a different class. It also requires a dictionary of hyperparameters, but instead of specifying a list of values to try, we specify a **probability distribution**. See the example below:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "param_distributions = {\n",
    "    'some_discete_hyperparameter': np.arange(1, 10), # uniform dicrete distribution from 1 to 10\n",
    "    'some_continuous_hyperparameter': uniform(0, 1), # uniform continuous distribution from 0 to 1\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions,\n",
    "    n_iter=100 # number of random samples to try\n",
    ")\n",
    "...\n",
    "```"
   ],
   "id": "1f84dd5deebbd0d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Saving and loading Python objects\n",
    "\n",
    "Both the featurizer and the trained regression model can be saved to disk as Python objects using the `pickle` module. This way, we can load them later in another script without the need to retrain the model or recalculate the molecular descriptors for fitting the feature scaler.\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "\n",
    "with open('path/to/model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Load the model\n",
    "\n",
    "with open('path/to/model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ],
   "id": "cc86bf1e4ca2c1d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Lab 4-2: Coding a simple app in Streamlit.\n",
    "\n",
    "`sreamlit` is a Python library that allows you to create very nice-looking and user-friendly web applications with just a few lines of code. It is very easy to use and requires absolutely no knowledge of web development.\n",
    "\n",
    "---\n",
    "\n",
    "We should have Streamlit already installed in our environment. You probably should take a look at the [Streamlit documentation](https://docs.streamlit.io/library), but this is how you can run a simple Streamlit app:\n",
    "\n",
    "Take look at `data/streamlit/app.py`. It is a tiny Streamlit app that takes a name as input and **returns a random fortune cookie-like quote**. Analyze the code and try to understand how it works. \n",
    "\n",
    "**Some important functions you may need to use:**\n",
    "\n",
    "- `st.title` sets the title of the app\n",
    "- `st.write` writes text to the app (supports [Markdown](https://www.markdownguide.org/basic-syntax/) synthax for text formatting)\n",
    "- `st.dataframe` displays a dataframe\n",
    "- `st.latex` renders LaTeX code (for mathematical formulas)\n",
    "- `st.text_input` creates a text input field (for multiline inputs, use `st.text_area`)\n",
    "- `st.button` creates a button (you can use it to trigger some action)\n",
    "\n",
    "To run the app, open a terminal and run the following command:\n",
    "\n",
    "    streamlit run data/streamlit/app.py\n",
    "    \n",
    "This will start a local server and open a new tab in your browser with the app. You can now interact with the app by entering your name and clicking the button to get a random fortune."
   ],
   "id": "3fb403d9e17203e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 5: Build an app for solubility prediction (3 points)\n",
    "\n",
    "This exercise is a continuation of the previous one. We will use all the code we have written so far to build a simple web app that predicts the solubility of molecules. The app will run in the browser and allow the user to input one or more SMILES strings, and will display the predicted solubility of the molecules.\n",
    "The [Streamlit documentation](https://docs.streamlit.io/library) will be very helpful in this task.\n",
    "\n",
    "**The app should have the following structure:**\n",
    "\n",
    "1. A title and a short description of what the app does.\n",
    "2. A text area where the user can input a SMILES string (or multiple SMILES strings in a column).\n",
    "3. A button to submit the input.\n",
    "4. A section that displays the predicted solubility of the molecule(s) as a table of SMILES strings and their corresponding solubility values.\n",
    "\n",
    "The app should use our `Featurizer` class to extract the features from the input SMILES strings and predict the solubility using the trained `svm` regressor. **The model and the featurizer should not be trained inside the app, but loaded from a `.pkl` file instead!** Thanks to this approach, we will not need the training data to run the app, and the app will start much faster, as it will not need to retrain the model every time."
   ],
   "id": "7f89948b93e1fa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# For this exercise, you should create a new Python script called in the 'data/streamlit' directory and implement the app there.",
   "id": "500173bbd37dfa41",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
